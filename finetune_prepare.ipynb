{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406c2014",
   "metadata": {},
   "source": [
    "## tsv genertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90cae7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ create new...\n",
      "processed 97 videos...\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_000.mp4 -> CamdenTown1_EQR_720p_190502_000\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_001.mp4 -> CamdenTown1_EQR_720p_190502_001\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_002.mp4 -> CamdenTown1_EQR_720p_190502_002\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_003.mp4 -> CamdenTown1_EQR_720p_190502_003\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_004.mp4 -> CamdenTown1_EQR_720p_190502_004\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_005.mp4 -> CamdenTown1_EQR_720p_190502_005\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_006.mp4 -> CamdenTown1_EQR_720p_190502_006\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_007.mp4 -> CamdenTown1_EQR_720p_190502_007\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_008.mp4 -> CamdenTown1_EQR_720p_190502_008\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_009.mp4 -> CamdenTown1_EQR_720p_190502_009\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_010.mp4 -> CamdenTown1_EQR_720p_190502_010\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_011.mp4 -> CamdenTown1_EQR_720p_190502_011\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_012.mp4 -> CamdenTown1_EQR_720p_190502_012\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_013.mp4 -> CamdenTown1_EQR_720p_190502_013\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_014.mp4 -> CamdenTown1_EQR_720p_190502_014\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_015.mp4 -> CamdenTown1_EQR_720p_190502_015\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_016.mp4 -> CamdenTown1_EQR_720p_190502_016\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_017.mp4 -> CamdenTown1_EQR_720p_190502_017\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_018.mp4 -> CamdenTown1_EQR_720p_190502_018\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_019.mp4 -> CamdenTown1_EQR_720p_190502_019\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_020.mp4 -> CamdenTown1_EQR_720p_190502_020\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_021.mp4 -> CamdenTown1_EQR_720p_190502_021\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_022.mp4 -> CamdenTown1_EQR_720p_190502_022\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_023.mp4 -> CamdenTown1_EQR_720p_190502_023\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_024.mp4 -> CamdenTown1_EQR_720p_190502_024\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_025.mp4 -> CamdenTown1_EQR_720p_190502_025\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_026.mp4 -> CamdenTown1_EQR_720p_190502_026\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_027.mp4 -> CamdenTown1_EQR_720p_190502_027\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_028.mp4 -> CamdenTown1_EQR_720p_190502_028\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_029.mp4 -> CamdenTown1_EQR_720p_190502_029\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_030.mp4 -> CamdenTown1_EQR_720p_190502_030\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_031.mp4 -> CamdenTown1_EQR_720p_190502_031\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_032.mp4 -> CamdenTown1_EQR_720p_190502_032\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_033.mp4 -> CamdenTown1_EQR_720p_190502_033\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_034.mp4 -> CamdenTown1_EQR_720p_190502_034\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_035.mp4 -> CamdenTown1_EQR_720p_190502_035\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_036.mp4 -> CamdenTown1_EQR_720p_190502_036\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_037.mp4 -> CamdenTown1_EQR_720p_190502_037\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_038.mp4 -> CamdenTown1_EQR_720p_190502_038\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_039.mp4 -> CamdenTown1_EQR_720p_190502_039\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_040.mp4 -> CamdenTown1_EQR_720p_190502_040\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_041.mp4 -> CamdenTown1_EQR_720p_190502_041\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_042.mp4 -> CamdenTown1_EQR_720p_190502_042\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_043.mp4 -> CamdenTown1_EQR_720p_190502_043\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_044.mp4 -> CamdenTown1_EQR_720p_190502_044\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_045.mp4 -> CamdenTown1_EQR_720p_190502_045\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_046.mp4 -> CamdenTown1_EQR_720p_190502_046\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_047.mp4 -> CamdenTown1_EQR_720p_190502_047\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_048.mp4 -> CamdenTown1_EQR_720p_190502_048\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_049.mp4 -> CamdenTown1_EQR_720p_190502_049\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_050.mp4 -> CamdenTown1_EQR_720p_190502_050\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_051.mp4 -> CamdenTown1_EQR_720p_190502_051\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_052.mp4 -> CamdenTown1_EQR_720p_190502_052\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_053.mp4 -> CamdenTown1_EQR_720p_190502_053\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_054.mp4 -> CamdenTown1_EQR_720p_190502_054\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_055.mp4 -> CamdenTown1_EQR_720p_190502_055\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_056.mp4 -> CamdenTown1_EQR_720p_190502_056\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_057.mp4 -> CamdenTown1_EQR_720p_190502_057\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_058.mp4 -> CamdenTown1_EQR_720p_190502_058\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_059.mp4 -> CamdenTown1_EQR_720p_190502_059\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_060.mp4 -> CamdenTown1_EQR_720p_190502_060\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_061.mp4 -> CamdenTown1_EQR_720p_190502_061\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_062.mp4 -> CamdenTown1_EQR_720p_190502_062\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_063.mp4 -> CamdenTown1_EQR_720p_190502_063\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_064.mp4 -> CamdenTown1_EQR_720p_190502_064\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_065.mp4 -> CamdenTown1_EQR_720p_190502_065\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_066.mp4 -> CamdenTown1_EQR_720p_190502_066\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_067.mp4 -> CamdenTown1_EQR_720p_190502_067\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_068.mp4 -> CamdenTown1_EQR_720p_190502_068\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_069.mp4 -> CamdenTown1_EQR_720p_190502_069\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_070.mp4 -> CamdenTown1_EQR_720p_190502_070\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_071.mp4 -> CamdenTown1_EQR_720p_190502_071\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_072.mp4 -> CamdenTown1_EQR_720p_190502_072\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_073.mp4 -> CamdenTown1_EQR_720p_190502_073\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_074.mp4 -> CamdenTown1_EQR_720p_190502_074\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_075.mp4 -> CamdenTown1_EQR_720p_190502_075\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_076.mp4 -> CamdenTown1_EQR_720p_190502_076\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_077.mp4 -> CamdenTown1_EQR_720p_190502_077\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_078.mp4 -> CamdenTown1_EQR_720p_190502_078\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_079.mp4 -> CamdenTown1_EQR_720p_190502_079\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_080.mp4 -> CamdenTown1_EQR_720p_190502_080\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_081.mp4 -> CamdenTown1_EQR_720p_190502_081\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_082.mp4 -> CamdenTown1_EQR_720p_190502_082\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_083.mp4 -> CamdenTown1_EQR_720p_190502_083\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_084.mp4 -> CamdenTown1_EQR_720p_190502_084\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_085.mp4 -> CamdenTown1_EQR_720p_190502_085\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_086.mp4 -> CamdenTown1_EQR_720p_190502_086\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_087.mp4 -> CamdenTown1_EQR_720p_190502_087\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_088.mp4 -> CamdenTown1_EQR_720p_190502_088\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_089.mp4 -> CamdenTown1_EQR_720p_190502_089\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_090.mp4 -> CamdenTown1_EQR_720p_190502_090\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_091.mp4 -> CamdenTown1_EQR_720p_190502_091\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_092.mp4 -> CamdenTown1_EQR_720p_190502_092\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_093.mp4 -> CamdenTown1_EQR_720p_190502_093\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_094.mp4 -> CamdenTown1_EQR_720p_190502_094\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_095.mp4 -> CamdenTown1_EQR_720p_190502_095\n",
      "  âœ“ CamdenTown1_EQR_720p_190502_096.mp4 -> CamdenTown1_EQR_720p_190502_096\n",
      "âœ… TSV file created at ./training/my_video_train.tsv\n",
      "Total videos processed: 97\n",
      "Total labels assigned: 97\n",
      "\n",
      "ğŸ“„ TSV:\n",
      "                                id             label\n",
      "0  CamdenTown1_EQR_720p_190502_000  audio from video\n",
      "1  CamdenTown1_EQR_720p_190502_001  audio from video\n",
      "2  CamdenTown1_EQR_720p_190502_002  audio from video\n",
      "3  CamdenTown1_EQR_720p_190502_003  audio from video\n",
      "4  CamdenTown1_EQR_720p_190502_004  audio from video\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_video_tsv_from_local():\n",
    "\n",
    "    video_dir = Path('./training_videos')\n",
    "    output_file = './training/my_video_train.tsv'\n",
    "\n",
    "    # make sure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    video_files = []\n",
    "    labels = []\n",
    "\n",
    "   # search for video files with common extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    all_videos = []\n",
    "\n",
    "    for ext in video_extensions:\n",
    "        all_videos.extend(video_dir.glob(f'*{ext}'))\n",
    "        all_videos.extend(video_dir.glob(f'*{ext.upper()}'))\n",
    "\n",
    "    print(f\"processed {len(all_videos)} videos...\")\n",
    "\n",
    "    for video_path in sorted(all_videos):\n",
    "        try:\n",
    "            # filter out non-video files\n",
    "            video_id = video_path.stem\n",
    "            video_files.append(video_id)\n",
    "\n",
    "            # assign a label for the video\n",
    "            label = \"audio from video\"  \n",
    "            labels.append(label)\n",
    "\n",
    "            print(f\"  âœ“ {video_path.name} -> {video_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ processed {video_path.name} error : {e}\")\n",
    "            continue\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"âŒ no video files found in the directory.\")\n",
    "        return None\n",
    "\n",
    "    # æ„å»º DataFrame\n",
    "    df = pd.DataFrame({'id': video_files, 'label': labels})\n",
    "\n",
    "    # ä¿å­˜åˆ°æœ¬åœ°\n",
    "    df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "    print(f\"âœ… TSV file created at {output_file}\")\n",
    "    print(f\"Total videos processed: {len(video_files)}\")\n",
    "    print(f\"Total labels assigned: {len(labels)}\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# check if the TSV file already exists\n",
    "local_tsv = Path('./training/my_video_train.tsv')\n",
    "\n",
    "if not local_tsv.exists():\n",
    "    print(\"ğŸ“‹ create new...\")\n",
    "    train_df = create_video_tsv_from_local()\n",
    "else:\n",
    "    print(\"ğŸ“‹ using tsv...\")\n",
    "    train_df = pd.read_csv(local_tsv, sep='\\t')\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"\\nğŸ“„ TSV:\")\n",
    "    print(train_df.head())\n",
    "else:\n",
    "    print(\"âŒ tsv file creation failed or no videos found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822ad32",
   "metadata": {},
   "source": [
    "## Download checkpoints and ext-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a439081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Setting up mirror environment for China users\n",
      "==================================================\n",
      "âœ… Set HF_ENDPOINT to: https://hf-mirror.com\n",
      "   HF_ENDPOINT = https://hf-mirror.com\n",
      "   HF_HUB_DISABLE_TELEMETRY = 1\n",
      "   HF_HUB_CACHE = /Users/yuqiliang/.cache/huggingface\n",
      "   CURL_TIMEOUT = 300\n",
      "\n",
      "ğŸ”§ Testing mirror connectivity...\n",
      "âœ… Mirror connection successful!\n",
      "\n",
      "ğŸ“ Creating directories...\n",
      "   â€¢ ext_weights: ext_weights\n",
      "   â€¢ weights: weights\n",
      "\n",
      "ğŸ“¦ Files to download from mirror:\n",
      "â€¢ v1-16.pth (60MB)\n",
      "  â””â”€ https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/v1-16.pth\n",
      "â€¢ synchformer_state_dict.pth (150MB)\n",
      "  â””â”€ https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/synchformer_state_dict.pth\n",
      "â€¢ best_netG.pt (40MB)\n",
      "  â””â”€ https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/best_netG.pt\n",
      "â€¢ mmaudio_small_16k.pth (600MB)\n",
      "  â””â”€ https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/weights/mmaudio_small_16k.pth\n",
      "\n",
      "ğŸ” Checking existing files...\n",
      "âŒ v1-16.pth missing\n",
      "âŒ synchformer_state_dict.pth missing\n",
      "âŒ best_netG.pt missing\n",
      "âŒ mmaudio_small_16k.pth missing\n",
      "\n",
      "ğŸš€ Downloading from mirror (hf-mirror.com)...\n",
      "=============================================\n",
      "\n",
      "ğŸ“¥ Downloading v1-16.pth\n",
      "   ğŸ“ VAE model for 16kHz (~60MB)\n",
      "   ğŸ”— https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/v1-16.pth\n",
      "   â³ Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%#=#=-#  #                                                                                                                                        3.0%                                                               4.7%                                                           16.4%##############################                                   55.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Success! Downloaded 654.8 MB\n",
      "\n",
      "ğŸ“¥ Downloading synchformer_state_dict.pth\n",
      "   ğŸ“ Synchronization model (~150MB)\n",
      "   ğŸ”— https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/synchformer_state_dict.pth\n",
      "   â³ Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%################                                          45.8%                62.9%########################################################         91.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Success! Downloaded 906.0 MB\n",
      "\n",
      "ğŸ“¥ Downloading best_netG.pt\n",
      "   ğŸ“ Vocoder model (~40MB)\n",
      "   ğŸ”— https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/ext_weights/best_netG.pt\n",
      "   â³ Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%#####                                                             18.1%################################################     96.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Success! Downloaded 428.4 MB\n",
      "\n",
      "ğŸ“¥ Downloading mmaudio_small_16k.pth\n",
      "   ğŸ“ MMAudio Small model (~600MB)\n",
      "   ğŸ”— https://hf-mirror.com/hkchengrex/MMAudio/resolve/main/weights/mmaudio_small_16k.pth\n",
      "   â³ Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################    98.4%########################                                 58.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Success! Downloaded 600.2 MB\n",
      "\n",
      "ğŸ Final Verification\n",
      "=========================\n",
      "âœ… v1-16.pth (654.8 MB)\n",
      "âœ… synchformer_state_dict.pth (906.0 MB)\n",
      "âœ… best_netG.pt (428.4 MB)\n",
      "âœ… mmaudio_small_16k.pth (600.2 MB)\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "   Success: 4/4 files\n",
      "   Total size: 2589.5 MB\n",
      "\n",
      "ğŸ‰ SUCCESS! All models downloaded\n",
      "âœ… Ready for feature extraction!\n",
      "\n",
      "ğŸ’¡ If downloads still fail, you may need to:\n",
      "   â€¢ Use a VPN\n",
      "   â€¢ Try different network\n",
      "   â€¢ Download manually from browser\n",
      "   â€¢ Ask someone else to download and share files\n",
      "\n",
      "ğŸ¯ Once all files are downloaded, you can run:\n",
      "   python3 training/extract_video_training_latents_mps.py \\\n",
      "     --latent_dir ./output/latents \\\n",
      "     --output_dir ./output/memmap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    }
   ],
   "source": [
    "# MMAudio Model Download with Mirror Support - Jupyter Notebook\n",
    "# ä½¿ç”¨é•œåƒæºä¸‹è½½æ¨¡å‹æ–‡ä»¶\n",
    "\n",
    "# Cell 1: è®¾ç½®é•œåƒç¯å¢ƒ\n",
    "print(\"ğŸŒ Setting up mirror environment for China users\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒ\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "print(\"âœ… Set HF_ENDPOINT to: https://hf-mirror.com\")\n",
    "\n",
    "# ä¹Ÿå¯ä»¥è®¾ç½®å…¶ä»–é•œåƒé€‰é¡¹\n",
    "mirror_options = {\n",
    "    'HF_ENDPOINT': 'https://hf-mirror.com',\n",
    "    'HF_HUB_DISABLE_TELEMETRY': '1',\n",
    "    'HF_HUB_CACHE': str(Path.home() / '.cache' / 'huggingface'),\n",
    "    'CURL_TIMEOUT': '300'  # 5åˆ†é’Ÿè¶…æ—¶\n",
    "}\n",
    "\n",
    "for key, value in mirror_options.items():\n",
    "    os.environ[key] = value\n",
    "    print(f\"   {key} = {value}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Testing mirror connectivity...\")\n",
    "test_result = subprocess.run(['curl', '-I', 'https://hf-mirror.com'], \n",
    "                           capture_output=True, text=True, timeout=10)\n",
    "if test_result.returncode == 0:\n",
    "    print(\"âœ… Mirror connection successful!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Mirror connection test failed, but proceeding...\")\n",
    "\n",
    "# Cell 2: åˆ›å»ºç›®å½•å’Œå®šä¹‰æ–‡ä»¶\n",
    "print(\"\\nğŸ“ Creating directories...\")\n",
    "\n",
    "# Create directories\n",
    "ext_weights_dir = Path(\"ext_weights\")\n",
    "weights_dir = Path(\"weights\")\n",
    "\n",
    "ext_weights_dir.mkdir(exist_ok=True)\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"   â€¢ ext_weights: {ext_weights_dir}\")\n",
    "print(f\"   â€¢ weights: {weights_dir}\")\n",
    "\n",
    "# ä½¿ç”¨é•œåƒæºçš„URL\n",
    "mirror_base_url = \"https://hf-mirror.com/hkchengrex/MMAudio/resolve/main\"\n",
    "\n",
    "small_model_files = {\n",
    "    f'{ext_weights_dir}/v1-16.pth': f'{mirror_base_url}/ext_weights/v1-16.pth',\n",
    "    f'{ext_weights_dir}/synchformer_state_dict.pth': f'{mirror_base_url}/ext_weights/synchformer_state_dict.pth',\n",
    "    f'{ext_weights_dir}/best_netG.pt': f'{mirror_base_url}/ext_weights/best_netG.pt',\n",
    "    f'{weights_dir}/mmaudio_small_16k.pth': f'{mirror_base_url}/weights/mmaudio_small_16k.pth'\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“¦ Files to download from mirror:\")\n",
    "file_info = {\n",
    "    'v1-16.pth': {'desc': 'VAE model for 16kHz', 'size_mb': 60},\n",
    "    'synchformer_state_dict.pth': {'desc': 'Synchronization model', 'size_mb': 150},\n",
    "    'best_netG.pt': {'desc': 'Vocoder model', 'size_mb': 40},\n",
    "    'mmaudio_small_16k.pth': {'desc': 'MMAudio Small model', 'size_mb': 600}\n",
    "}\n",
    "\n",
    "for file_path, url in small_model_files.items():\n",
    "    filename = Path(file_path).name\n",
    "    info = file_info[filename]\n",
    "    print(f\"â€¢ {filename} ({info['size_mb']}MB)\")\n",
    "    print(f\"  â””â”€ {url}\")\n",
    "\n",
    "# Cell 3: æ£€æŸ¥ç°æœ‰æ–‡ä»¶\n",
    "print(f\"\\nğŸ” Checking existing files...\")\n",
    "existing_files = []\n",
    "missing_files = []\n",
    "\n",
    "for file_path, url in small_model_files.items():\n",
    "    path = Path(file_path)\n",
    "    if path.exists() and path.stat().st_size > 1024*1024:\n",
    "        size_mb = path.stat().st_size / (1024*1024)\n",
    "        print(f\"âœ… {path.name} exists ({size_mb:.1f} MB)\")\n",
    "        existing_files.append(file_path)\n",
    "    else:\n",
    "        print(f\"âŒ {path.name} missing\")\n",
    "        missing_files.append(file_path)\n",
    "\n",
    "if not missing_files:\n",
    "    print(\"\\nğŸ‰ All files already downloaded!\")\n",
    "\n",
    "# Cell 4: ä½¿ç”¨é•œåƒä¸‹è½½\n",
    "if missing_files:\n",
    "    print(f\"\\nğŸš€ Downloading from mirror (hf-mirror.com)...\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    for file_path in missing_files:\n",
    "        path = Path(file_path)\n",
    "        filename = path.name\n",
    "        url = small_model_files[file_path]\n",
    "        info = file_info[filename]\n",
    "        \n",
    "        print(f\"\\nğŸ“¥ Downloading {filename}\")\n",
    "        print(f\"   ğŸ“ {info['desc']} (~{info['size_mb']}MB)\")\n",
    "        print(f\"   ğŸ”— {url}\")\n",
    "        \n",
    "        # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # ä½¿ç”¨curlä¸‹è½½ï¼Œå¢åŠ æ›´å¤šé€‰é¡¹\n",
    "        curl_command = [\n",
    "            'curl', \n",
    "            '-L',                    # è·Ÿéšé‡å®šå‘\n",
    "            '-o', str(path),         # è¾“å‡ºæ–‡ä»¶\n",
    "            '--connect-timeout', '30', # è¿æ¥è¶…æ—¶30ç§’\n",
    "            '--max-time', '1200',    # æœ€å¤§ä¼ è¾“æ—¶é—´20åˆ†é’Ÿ\n",
    "            '--retry', '3',          # é‡è¯•3æ¬¡\n",
    "            '--retry-delay', '5',    # é‡è¯•é—´éš”5ç§’\n",
    "            '--progress-bar',        # æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "            url\n",
    "        ]\n",
    "        \n",
    "        print(f\"   â³ Starting download...\")\n",
    "        result = subprocess.run(curl_command)\n",
    "        \n",
    "        # æ£€æŸ¥ç»“æœ\n",
    "        if result.returncode == 0 and path.exists():\n",
    "            size_mb = path.stat().st_size / (1024*1024)\n",
    "            if size_mb > 1:\n",
    "                print(f\"   âœ… Success! Downloaded {size_mb:.1f} MB\")\n",
    "            else:\n",
    "                print(f\"   âŒ File too small ({size_mb:.1f} MB) - may be corrupted\")\n",
    "        else:\n",
    "            print(f\"   âŒ Download failed (return code: {result.returncode})\")\n",
    "            \n",
    "            # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–é•œåƒ\n",
    "            if result.returncode == 28:  # è¶…æ—¶é”™è¯¯\n",
    "                print(f\"   ğŸ”„ Timeout error, trying alternative method...\")\n",
    "                \n",
    "                # å°è¯•åˆ†æ®µä¸‹è½½\n",
    "                alt_command = [\n",
    "                    'curl', '-L', '-C', '-',  # ç»­ä¼ \n",
    "                    '-o', str(path), \n",
    "                    '--connect-timeout', '60',\n",
    "                    '--max-time', '3600',  # 1å°æ—¶\n",
    "                    '--retry', '5',\n",
    "                    url\n",
    "                ]\n",
    "                \n",
    "                alt_result = subprocess.run(alt_command)\n",
    "                if alt_result.returncode == 0 and path.exists():\n",
    "                    size_mb = path.stat().st_size / (1024*1024)\n",
    "                    print(f\"   âœ… Alternative download successful! {size_mb:.1f} MB\")\n",
    "                else:\n",
    "                    print(f\"   âŒ Alternative download also failed\")\n",
    "\n",
    "# Cell 5: æœ€ç»ˆéªŒè¯\n",
    "print(f\"\\nğŸ Final Verification\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "success_count = 0\n",
    "total_size_mb = 0\n",
    "\n",
    "for file_path, url in small_model_files.items():\n",
    "    path = Path(file_path)\n",
    "    if path.exists() and path.stat().st_size > 1024*1024:\n",
    "        size_mb = path.stat().st_size / (1024*1024)\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"âœ… {path.name} ({size_mb:.1f} MB)\")\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(f\"âŒ {path.name} - missing or corrupted\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"   Success: {success_count}/{len(small_model_files)} files\")\n",
    "print(f\"   Total size: {total_size_mb:.1f} MB\")\n",
    "\n",
    "if success_count == len(small_model_files):\n",
    "    print(f\"\\nğŸ‰ SUCCESS! All models downloaded\")\n",
    "    print(f\"âœ… Ready for feature extraction!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  {len(small_model_files) - success_count} files still missing\")\n",
    "\n",
    "# Cell 6: å¤‡ç”¨ä¸‹è½½æ–¹æ³• (å¦‚æœé•œåƒä¹Ÿå¤±è´¥)\n",
    "if success_count < len(small_model_files):\n",
    "    print(f\"\\nğŸ”§ Alternative Download Methods\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"If mirror download failed, try these alternatives:\")\n",
    "    print()\n",
    "    print(\"1. æ‰‹åŠ¨ä¸‹è½½ (Manual Download):\")\n",
    "    print(\"   Visit https://hf-mirror.com/hkchengrex/MMAudio/tree/main\")\n",
    "    print(\"   Download files manually to correct directories\")\n",
    "    print()\n",
    "    print(\"2. ä½¿ç”¨ä»£ç† (Use Proxy):\")\n",
    "    print(\"   export https_proxy=http://your-proxy:port\")\n",
    "    print(\"   export http_proxy=http://your-proxy:port\")\n",
    "    print()\n",
    "    print(\"3. åˆ†æ®µä¸‹è½½ (Resume Download):\")\n",
    "    \n",
    "    for file_path, url in small_model_files.items():\n",
    "        path = Path(file_path)\n",
    "        if not (path.exists() and path.stat().st_size > 1024*1024):\n",
    "            print(f\"   curl -C - -L -o {file_path} \\\\\")\n",
    "            print(f\"        {url}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"4. ä½¿ç”¨Python requests:\")\n",
    "    print(\"   (See next cell for Python download method)\")\n",
    "\n",
    "# Cell 7: Pythonå¤‡ç”¨ä¸‹è½½æ–¹æ³•\n",
    "def python_download_fallback():\n",
    "    \"\"\"ä½¿ç”¨Python requestsä½œä¸ºå¤‡ç”¨ä¸‹è½½æ–¹æ³•\"\"\"\n",
    "    import requests\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"ğŸ Python Download Fallback Method\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for file_path, url in small_model_files.items():\n",
    "        path = Path(file_path)\n",
    "        if path.exists() and path.stat().st_size > 1024*1024:\n",
    "            continue  # è·³è¿‡å·²ä¸‹è½½çš„æ–‡ä»¶\n",
    "            \n",
    "        filename = path.name\n",
    "        print(f\"\\nğŸ“¥ Trying Python download: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # ä¿®æ”¹URLä¸ºé•œåƒ\n",
    "            response = requests.get(url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(path, 'wb') as f:\n",
    "                if total_size > 0:\n",
    "                    with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                                pbar.update(len(chunk))\n",
    "                else:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            \n",
    "            if path.exists() and path.stat().st_size > 1024*1024:\n",
    "                size_mb = path.stat().st_size / (1024*1024)\n",
    "                print(f\"   âœ… Python download success: {size_mb:.1f} MB\")\n",
    "            else:\n",
    "                print(f\"   âŒ Python download failed - file too small\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Python download error: {e}\")\n",
    "\n",
    "# å¦‚æœéœ€è¦ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œæ¥è¿è¡ŒPythonå¤‡ç”¨ä¸‹è½½\n",
    "# python_download_fallback()\n",
    "\n",
    "print(f\"\\nğŸ’¡ If downloads still fail, you may need to:\")\n",
    "print(f\"   â€¢ Use a VPN\")\n",
    "print(f\"   â€¢ Try different network\")\n",
    "print(f\"   â€¢ Download manually from browser\")\n",
    "print(f\"   â€¢ Ask someone else to download and share files\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Once all files are downloaded, you can run:\")\n",
    "print(f\"   python3 training/extract_video_training_latents_mps.py \\\\\")\n",
    "print(f\"     --latent_dir ./output/latents \\\\\")\n",
    "print(f\"     --output_dir ./output/memmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a755a0f",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f79f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” MMAudio Local Feature Extraction Status Check\n",
      "==================================================\n",
      "ğŸ” Checking local feature extraction results...\n",
      "   Output directory: output/memmap\n",
      "   Feature directory: output/memmap/vgg-my_videos\n",
      "\n",
      "ğŸ“ Checking feature files in: output/memmap/vgg-my_videos\n",
      "   âœ… mean.memmap (1.8 MB)\n",
      "   âœ… std.memmap (1.8 MB)\n",
      "   âœ… clip_features.memmap (24.0 MB)\n",
      "   âœ… sync_features.memmap (54.0 MB)\n",
      "   âœ… text_features.memmap (28.9 MB)\n",
      "   âœ… meta.json\n",
      "\n",
      "ğŸ“„ Checking TSV file: output/memmap/vgg-my_videos.tsv\n",
      "   âœ… vgg-my_videos.tsv exists\n",
      "   ğŸ“Š Contains 96 video samples\n",
      "   ğŸ“‹ Sample entries:\n",
      "      1. CamdenTown1_EQR_720p_190502_000: audio from video\n",
      "      2. CamdenTown1_EQR_720p_190502_001: audio from video\n",
      "      3. CamdenTown1_EQR_720p_190502_002: audio from video\n",
      "      ... and 93 more\n",
      "\n",
      "ğŸ‰ Feature extraction completed successfully!\n",
      "   ğŸ“Š Total feature size: 110.5 MB\n",
      "   ğŸ“ Features saved locally\n",
      "   âœ… Ready for training!\n",
      "\n",
      "ğŸ” Checking intermediate files...\n",
      "   ğŸ“ Latent directory: 0 batch files\n",
      "\n",
      "ğŸ¯ Next Steps:\n",
      "   ğŸ‰ Feature extraction is complete!\n",
      "   ğŸš€ You can proceed with training:\n",
      "      python3 train.py exp_id=my_fine_tuning model=small_16k\n",
      "\n",
      "==================================================\n",
      "âœ… Status: Feature extraction COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Check Local MMAudio Feature Extraction Results\n",
    "==============================================\n",
    "Checks the status of feature extraction on local MacBook\n",
    "\"\"\"\n",
    "\n",
    "def check_extraction_results():\n",
    "    \"\"\"Check local feature extraction results\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Local paths\n",
    "    output_dir = Path('./output/memmap')\n",
    "    feature_dir = output_dir / 'vgg-my_videos'  # Note: changed to vgg-my_videos\n",
    "    \n",
    "    print(f\"ğŸ” Checking local feature extraction results...\")\n",
    "    print(f\"   Output directory: {output_dir}\")\n",
    "    print(f\"   Feature directory: {feature_dir}\")\n",
    "    \n",
    "    # Check if output directory exists\n",
    "    if not output_dir.exists():\n",
    "        print(\"âŒ Output directory does not exist\")\n",
    "        print(f\"   Expected: {output_dir}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if feature directory exists\n",
    "    if not feature_dir.exists():\n",
    "        print(\"âŒ Feature directory does not exist\")\n",
    "        print(f\"   Expected: {feature_dir}\")\n",
    "        \n",
    "        # Check what directories do exist\n",
    "        existing_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "        if existing_dirs:\n",
    "            print(f\"   ğŸ” Found these directories instead:\")\n",
    "            for d in existing_dirs:\n",
    "                print(f\"      â€¢ {d.name}\")\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    # Check required feature files\n",
    "    required_features = ['mean.memmap', 'std.memmap', 'clip_features.memmap', \n",
    "                        'sync_features.memmap', 'text_features.memmap', 'meta.json']\n",
    "    \n",
    "    print(f\"\\nğŸ“ Checking feature files in: {feature_dir}\")\n",
    "    all_exist = True\n",
    "    total_size_mb = 0\n",
    "    \n",
    "    for feature_file in required_features:\n",
    "        file_path = feature_dir / feature_file\n",
    "        if file_path.exists():\n",
    "            if feature_file.endswith('.memmap'):\n",
    "                size_mb = file_path.stat().st_size / (1024*1024)\n",
    "                total_size_mb += size_mb\n",
    "                print(f\"   âœ… {feature_file} ({size_mb:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"   âœ… {feature_file}\")\n",
    "        else:\n",
    "            print(f\"   âŒ {feature_file} - missing\")\n",
    "            all_exist = False\n",
    "    \n",
    "    # Check TSV file\n",
    "    tsv_file = output_dir / 'vgg-my_videos.tsv'  # Note: changed to vgg-my_videos\n",
    "    print(f\"\\nğŸ“„ Checking TSV file: {tsv_file}\")\n",
    "    \n",
    "    if tsv_file.exists():\n",
    "        print(f\"   âœ… vgg-my_videos.tsv exists\")\n",
    "        \n",
    "        # Read and display statistics\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(tsv_file, sep='\\t')\n",
    "            print(f\"   ğŸ“Š Contains {len(df)} video samples\")\n",
    "            \n",
    "            # Show sample data\n",
    "            if len(df) > 0:\n",
    "                print(f\"   ğŸ“‹ Sample entries:\")\n",
    "                for i, row in df.head(3).iterrows():\n",
    "                    print(f\"      {i+1}. {row['id']}: {row['label']}\")\n",
    "                if len(df) > 3:\n",
    "                    print(f\"      ... and {len(df)-3} more\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error reading TSV: {e}\")\n",
    "            all_exist = False\n",
    "    else:\n",
    "        print(f\"   âŒ vgg-my_videos.tsv - missing\")\n",
    "        all_exist = False\n",
    "    \n",
    "    # Final status\n",
    "    if all_exist:\n",
    "        print(f\"\\nğŸ‰ Feature extraction completed successfully!\")\n",
    "        print(f\"   ğŸ“Š Total feature size: {total_size_mb:.1f} MB\")\n",
    "        print(f\"   ğŸ“ Features saved locally\")\n",
    "        print(f\"   âœ… Ready for training!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\nâŒ Feature extraction incomplete\")\n",
    "        return False\n",
    "\n",
    "def check_intermediate_files():\n",
    "    \"\"\"Check intermediate extraction files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"\\nğŸ” Checking intermediate files...\")\n",
    "    \n",
    "    # Check latent directory\n",
    "    latent_dir = Path('./output/latents/my_videos')\n",
    "    if latent_dir.exists():\n",
    "        batch_files = list(latent_dir.glob('*.pth'))\n",
    "        print(f\"   ğŸ“ Latent directory: {len(batch_files)} batch files\")\n",
    "        \n",
    "        if batch_files:\n",
    "            # Show range of files\n",
    "            file_numbers = []\n",
    "            for f in batch_files:\n",
    "                try:\n",
    "                    # Extract number from filename like r0_42.pth\n",
    "                    parts = f.stem.split('_')\n",
    "                    if len(parts) >= 2:\n",
    "                        file_numbers.append(int(parts[1]))\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if file_numbers:\n",
    "                min_batch = min(file_numbers)\n",
    "                max_batch = max(file_numbers)\n",
    "                print(f\"      Batch range: {min_batch} to {max_batch}\")\n",
    "                print(f\"      Expected total batches: ~49 (based on 97 videos / 2 batch_size)\")\n",
    "                \n",
    "                if max_batch >= 47:  # Close to expected 49 batches\n",
    "                    print(f\"      âœ… Most batches processed\")\n",
    "                else:\n",
    "                    print(f\"      âš ï¸  Only reached batch {max_batch}\")\n",
    "    else:\n",
    "        print(f\"   âŒ No latent directory found\")\n",
    "\n",
    "def show_next_steps():\n",
    "    \"\"\"Show next steps based on current status\"\"\"\n",
    "    print(f\"\\nğŸ¯ Next Steps:\")\n",
    "    \n",
    "    # Check what exists\n",
    "    output_dir = Path('./output/memmap')\n",
    "    feature_dir = output_dir / 'vgg-my_videos'\n",
    "    latent_dir = Path('./output/latents/my_videos')\n",
    "    \n",
    "    if feature_dir.exists():\n",
    "        print(f\"   ğŸ‰ Feature extraction is complete!\")\n",
    "        print(f\"   ğŸš€ You can proceed with training:\")\n",
    "        print(f\"      python3 train.py exp_id=my_fine_tuning model=small_16k\")\n",
    "        \n",
    "    elif latent_dir.exists() and len(list(latent_dir.glob('*.pth'))) > 40:\n",
    "        print(f\"   ğŸ”„ Feature extraction was mostly completed but needs finishing\")\n",
    "        print(f\"   ğŸ’¡ Try running extraction again - it should complete quickly:\")\n",
    "        print(f\"      python3 training/extract_video_training_latents_mps.py \\\\\")\n",
    "        print(f\"        --latent_dir ./output/latents \\\\\")\n",
    "        print(f\"        --output_dir ./output/memmap\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   âŒ Feature extraction needs to be run\")\n",
    "        print(f\"   ğŸ”§ First fix the problematic video:\")\n",
    "        print(f\"      mv training_videos/CamdenTown1_EQR_720p_190502_096.mp4 training_videos_problematic/\")\n",
    "        print(f\"   ğŸ“ Update TSV file to remove the problematic entry\")\n",
    "        print(f\"   ğŸš€ Then run extraction:\")\n",
    "        print(f\"      python3 training/extract_video_training_latents_mps.py \\\\\")\n",
    "        print(f\"        --latent_dir ./output/latents \\\\\")\n",
    "        print(f\"        --output_dir ./output/memmap\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to check extraction status\"\"\"\n",
    "    print(\"ğŸ” MMAudio Local Feature Extraction Status Check\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check final results\n",
    "    extraction_success = check_extraction_results()\n",
    "    \n",
    "    # Check intermediate files\n",
    "    check_intermediate_files()\n",
    "    \n",
    "    # Show next steps\n",
    "    show_next_steps()\n",
    "    \n",
    "    return extraction_success\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    if success:\n",
    "        print(\"âœ… Status: Feature extraction COMPLETE\")\n",
    "    else:\n",
    "        print(\"â³ Status: Feature extraction INCOMPLETE\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0adb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” MMAudio è®­ç»ƒæ•°æ®å®Œæ•´æ€§æ£€æŸ¥\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“ è§†é¢‘æ–‡ä»¶æ£€æŸ¥\n",
      "============================================================\n",
      "è§†é¢‘ç›®å½•                          : training_videos\n",
      "è§†é¢‘æ–‡ä»¶æ•°é‡                        : 20\n",
      "\n",
      "è§†é¢‘æ–‡ä»¶åˆ—è¡¨:\n",
      "  1. CamdenTown1_EQR_720p_190502_007.mp4 (9.1 MB)\n",
      "  2. CamdenTown1_EQR_720p_190502_013.mp4 (9.0 MB)\n",
      "  3. CamdenTown1_EQR_720p_190502_012.mp4 (8.8 MB)\n",
      "  4. CamdenTown1_EQR_720p_190502_006.mp4 (9.3 MB)\n",
      "  5. CamdenTown1_EQR_720p_190502_010.mp4 (8.8 MB)\n",
      "  6. CamdenTown1_EQR_720p_190502_004.mp4 (8.8 MB)\n",
      "  7. CamdenTown1_EQR_720p_190502_005.mp4 (9.1 MB)\n",
      "  8. CamdenTown1_EQR_720p_190502_011.mp4 (9.2 MB)\n",
      "  9. CamdenTown1_EQR_720p_190502_015.mp4 (9.1 MB)\n",
      "  10. CamdenTown1_EQR_720p_190502_001.mp4 (8.0 MB)\n",
      "  ... è¿˜æœ‰ 10 ä¸ªæ–‡ä»¶\n",
      "\n",
      "============================================================\n",
      "ğŸ“„ TSVå…ƒæ•°æ®æ–‡ä»¶æ£€æŸ¥\n",
      "============================================================\n",
      "TSVæ–‡ä»¶è·¯å¾„                       : training/my_video_train.tsv\n",
      "æ€»æ ·æœ¬æ•°                          : 97\n",
      "åˆ—å                            : ['id', 'label']\n",
      "å”¯ä¸€IDæ•°é‡                        : 97\n",
      "\n",
      "å‰5ä¸ªæ ‡ç­¾:\n",
      "  1. audio from video...\n",
      "  2. audio from video...\n",
      "  3. audio from video...\n",
      "  4. audio from video...\n",
      "  5. audio from video...\n",
      "\n",
      "============================================================\n",
      "ğŸ’¾ Memory-Mapped Tensoræ£€æŸ¥\n",
      "============================================================\n",
      "åŠ è½½æ•°æ®ä»: output/memmap/vgg-my_videos\n",
      "å¯ç”¨çš„ç‰¹å¾                         : ['mean', 'std', 'clip_features', 'sync_features', 'text_features']\n",
      "\n",
      "ç‰¹å¾ç»´åº¦:\n",
      "  mean                        : torch.Size([96, 250, 20]) (dtype: torch.float32)\n",
      "  std                         : torch.Size([96, 250, 20]) (dtype: torch.float32)\n",
      "  clip_features               : torch.Size([96, 64, 1024]) (dtype: torch.float32)\n",
      "  sync_features               : torch.Size([96, 192, 768]) (dtype: torch.float32)\n",
      "  text_features               : torch.Size([96, 77, 1024]) (dtype: torch.float32)\n",
      "\n",
      "æ€»æ ·æœ¬æ•°                         : 96\n",
      "\n",
      "æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥:\n",
      "  mean åŒ…å«NaN                  : å¦ âœ…\n",
      "  mean åŒ…å«Inf                  : å¦ âœ…\n",
      "  std åŒ…å«NaN                   : å¦ âœ…\n",
      "  std åŒ…å«Inf                   : å¦ âœ…\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š æ•°æ®é›†æ€»ç»“\n",
      "============================================================\n",
      "âœ… æ‰¾åˆ° 20 ä¸ªè§†é¢‘æ–‡ä»¶\n",
      "âœ… TSVæ–‡ä»¶åŒ…å« 97 æ¡è®°å½•\n",
      "âœ… æå–äº† 96 ä¸ªæ ·æœ¬çš„ç‰¹å¾\n",
      "\n",
      "============================================================\n",
      "å¦‚æœéœ€è¦åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œè¯·è¿è¡Œ:\n",
      "python check_dataset.py --create-dummy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "æ£€æŸ¥MMAudioæ•°æ®é›†æ ·æœ¬æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯\n",
    "ç”¨äºè¯Šæ–­è®­ç»ƒæ•°æ®é—®é¢˜\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensordict as td\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def print_header(text):\n",
    "    \"\"\"æ‰“å°æ ‡é¢˜\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{text}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def print_info(label, value):\n",
    "    \"\"\"æ‰“å°ä¿¡æ¯\"\"\"\n",
    "    print(f\"{label:<30}: {value}\")\n",
    "\n",
    "def check_video_directory(video_dir):\n",
    "    \"\"\"æ£€æŸ¥è§†é¢‘ç›®å½•\"\"\"\n",
    "    print_header(\"ğŸ“ è§†é¢‘æ–‡ä»¶æ£€æŸ¥\")\n",
    "    \n",
    "    video_dir = Path(video_dir)\n",
    "    if not video_dir.exists():\n",
    "        print(f\"âŒ ç›®å½•ä¸å­˜åœ¨: {video_dir}\")\n",
    "        print(f\"   è¯·åˆ›å»ºç›®å½•å¹¶æ·»åŠ è§†é¢‘æ–‡ä»¶\")\n",
    "        return 0\n",
    "    \n",
    "    # æ”¯æŒçš„è§†é¢‘æ ¼å¼\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.m4v', '.flv']\n",
    "    video_files = []\n",
    "    \n",
    "    for ext in video_extensions:\n",
    "        files = list(video_dir.glob(f'*{ext}')) + list(video_dir.glob(f'*{ext.upper()}'))\n",
    "        video_files.extend(files)\n",
    "    \n",
    "    print_info(\"è§†é¢‘ç›®å½•\", str(video_dir))\n",
    "    print_info(\"è§†é¢‘æ–‡ä»¶æ•°é‡\", len(video_files))\n",
    "    \n",
    "    if video_files:\n",
    "        print(f\"\\nè§†é¢‘æ–‡ä»¶åˆ—è¡¨:\")\n",
    "        for i, video in enumerate(video_files[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª\n",
    "            size_mb = video.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {i}. {video.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        if len(video_files) > 10:\n",
    "            print(f\"  ... è¿˜æœ‰ {len(video_files)-10} ä¸ªæ–‡ä»¶\")\n",
    "    else:\n",
    "        print(\"âš ï¸  æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶!\")\n",
    "        print(f\"   æ”¯æŒçš„æ ¼å¼: {', '.join(video_extensions)}\")\n",
    "    \n",
    "    return len(video_files)\n",
    "\n",
    "def check_tsv_file(tsv_path):\n",
    "    \"\"\"æ£€æŸ¥TSVæ–‡ä»¶\"\"\"\n",
    "    print_header(\"ğŸ“„ TSVå…ƒæ•°æ®æ–‡ä»¶æ£€æŸ¥\")\n",
    "    \n",
    "    tsv_path = Path(tsv_path)\n",
    "    if not tsv_path.exists():\n",
    "        print(f\"âŒ TSVæ–‡ä»¶ä¸å­˜åœ¨: {tsv_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        \n",
    "        print_info(\"TSVæ–‡ä»¶è·¯å¾„\", str(tsv_path))\n",
    "        print_info(\"æ€»æ ·æœ¬æ•°\", len(df))\n",
    "        print_info(\"åˆ—å\", list(df.columns))\n",
    "        \n",
    "        if 'id' in df.columns:\n",
    "            print_info(\"å”¯ä¸€IDæ•°é‡\", df['id'].nunique())\n",
    "            \n",
    "            # æ£€æŸ¥é‡å¤\n",
    "            duplicates = df['id'].duplicated().sum()\n",
    "            if duplicates > 0:\n",
    "                print_info(\"é‡å¤IDæ•°é‡\", duplicates)\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            # æ˜¾ç¤ºå‰5ä¸ªæ ‡ç­¾\n",
    "            print(\"\\nå‰5ä¸ªæ ‡ç­¾:\")\n",
    "            for i, label in enumerate(df['label'].head(5), 1):\n",
    "                print(f\"  {i}. {label[:100]}...\")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–TSVæ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_memmap_data(memmap_dir):\n",
    "    \"\"\"æ£€æŸ¥memory-mapped tensoræ•°æ®\"\"\"\n",
    "    print_header(\"ğŸ’¾ Memory-Mapped Tensoræ£€æŸ¥\")\n",
    "    \n",
    "    memmap_dir = Path(memmap_dir)\n",
    "    if not memmap_dir.exists():\n",
    "        print(f\"âŒ Memmapç›®å½•ä¸å­˜åœ¨: {memmap_dir}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½tensor dict\n",
    "        print(f\"åŠ è½½æ•°æ®ä»: {memmap_dir}\")\n",
    "        tensor_dict = td.TensorDict.load_memmap(memmap_dir)\n",
    "        \n",
    "        # æ£€æŸ¥keys\n",
    "        keys = list(tensor_dict.keys())\n",
    "        print_info(\"å¯ç”¨çš„ç‰¹å¾\", keys)\n",
    "        \n",
    "        # æ£€æŸ¥æ¯ä¸ªtensorçš„å½¢çŠ¶\n",
    "        print(\"\\nç‰¹å¾ç»´åº¦:\")\n",
    "        for key in keys:\n",
    "            tensor = tensor_dict[key]\n",
    "            print_info(f\"  {key}\", f\"{tensor.shape} (dtype: {tensor.dtype})\")\n",
    "        \n",
    "        # è·å–æ ·æœ¬æ•°é‡\n",
    "        if 'mean' in keys:\n",
    "            num_samples = len(tensor_dict['mean'])\n",
    "            print_info(\"\\næ€»æ ·æœ¬æ•°\", num_samples)\n",
    "            \n",
    "            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§\n",
    "            print(\"\\næ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥:\")\n",
    "            for key in ['mean', 'std']:\n",
    "                if key in keys:\n",
    "                    tensor = tensor_dict[key]\n",
    "                    has_nan = torch.isnan(tensor).any().item()\n",
    "                    has_inf = torch.isinf(tensor).any().item()\n",
    "                    print_info(f\"  {key} åŒ…å«NaN\", \"æ˜¯ âš ï¸\" if has_nan else \"å¦ âœ…\")\n",
    "                    print_info(f\"  {key} åŒ…å«Inf\", \"æ˜¯ âš ï¸\" if has_inf else \"å¦ âœ…\")\n",
    "            \n",
    "            return num_samples\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½memmapæ•°æ®å¤±è´¥: {e}\")\n",
    "        print(f\"   é”™è¯¯è¯¦æƒ…: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def check_training_data():\n",
    "    \"\"\"æ£€æŸ¥æ‰€æœ‰è®­ç»ƒæ•°æ®\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ” MMAudio è®­ç»ƒæ•°æ®å®Œæ•´æ€§æ£€æŸ¥\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. æ£€æŸ¥åŸå§‹è§†é¢‘ç›®å½•\n",
    "    video_count = check_video_directory('./training_videos')\n",
    "    \n",
    "    # 2. æ£€æŸ¥TSVæ–‡ä»¶ï¼ˆä¸¤ä¸ªå¯èƒ½çš„ä½ç½®ï¼‰\n",
    "    tsv_paths = [\n",
    "        './training/my_video_train.tsv',  # åŸå§‹ä½ç½®\n",
    "        './output/memmap/vgg-my_videos.tsv'  # ç”Ÿæˆåçš„ä½ç½®\n",
    "    ]\n",
    "    \n",
    "    tsv_df = None\n",
    "    for tsv_path in tsv_paths:\n",
    "        if Path(tsv_path).exists():\n",
    "            tsv_df = check_tsv_file(tsv_path)\n",
    "            break\n",
    "    \n",
    "    if tsv_df is None:\n",
    "        print(\"\\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°TSVæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œç‰¹å¾æå–\")\n",
    "    \n",
    "    # 3. æ£€æŸ¥æå–çš„ç‰¹å¾\n",
    "    memmap_dirs = [\n",
    "        './output/memmap/vgg-my_videos',\n",
    "        './fine_tuning_data/output/memmap/vgg-my_videos'\n",
    "    ]\n",
    "    \n",
    "    num_samples = None\n",
    "    for memmap_dir in memmap_dirs:\n",
    "        if Path(memmap_dir).exists():\n",
    "            num_samples = check_memmap_data(memmap_dir)\n",
    "            break\n",
    "    \n",
    "    if num_samples is None:\n",
    "        print(\"\\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°æå–çš„ç‰¹å¾ï¼Œè¯·å…ˆè¿è¡Œç‰¹å¾æå–è„šæœ¬\")\n",
    "    \n",
    "    # 4. æ€»ç»“\n",
    "    print_header(\"ğŸ“Š æ•°æ®é›†æ€»ç»“\")\n",
    "    \n",
    "    if video_count > 0:\n",
    "        print(f\"âœ… æ‰¾åˆ° {video_count} ä¸ªè§†é¢‘æ–‡ä»¶\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶\")\n",
    "    \n",
    "    if tsv_df is not None:\n",
    "        print(f\"âœ… TSVæ–‡ä»¶åŒ…å« {len(tsv_df)} æ¡è®°å½•\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°TSVæ–‡ä»¶\")\n",
    "    \n",
    "    if num_samples is not None:\n",
    "        print(f\"âœ… æå–äº† {num_samples} ä¸ªæ ·æœ¬çš„ç‰¹å¾\")\n",
    "        \n",
    "        if num_samples < 2:\n",
    "            print(\"\\nâš ï¸  è­¦å‘Š: æ ·æœ¬æ•°é‡å¤ªå°‘!\")\n",
    "            print(\"   è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½è®­ç»ƒï¼ˆbatch_size=2ï¼‰\")\n",
    "            print(\"\\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆ:\")\n",
    "            print(\"   1. æ·»åŠ æ›´å¤šè§†é¢‘åˆ° ./training_videos/ ç›®å½•\")\n",
    "            print(\"   2. é‡æ–°è¿è¡Œç‰¹å¾æå–è„šæœ¬\")\n",
    "            print(\"   3. æˆ–è€…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ --batch_size 1\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°æå–çš„ç‰¹å¾\")\n",
    "        print(\"\\nè¯·è¿è¡Œç‰¹å¾æå–è„šæœ¬:\")\n",
    "        print(\"   python3 extract_video_training_latents_mps.py \\\\\")\n",
    "        print(\"     --latent_dir ./output/latents \\\\\")\n",
    "        print(\"     --output_dir ./output/memmap\")\n",
    "    \n",
    "    # 5. æä¾›ä¿®å¤å»ºè®®\n",
    "    if num_samples == 1:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”§ ä¿®å¤å»ºè®®\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\næ‚¨åªæœ‰1ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæœ‰ä»¥ä¸‹å‡ ç§è§£å†³æ–¹æ¡ˆ:\")\n",
    "        print(\"\\næ–¹æ¡ˆ1: æ·»åŠ æ›´å¤šè§†é¢‘\")\n",
    "        print(\"   1. å°†æ›´å¤šè§†é¢‘æ–‡ä»¶æ”¾å…¥ ./training_videos/\")\n",
    "        print(\"   2. é‡æ–°è¿è¡Œç‰¹å¾æå–è„šæœ¬\")\n",
    "        print(\"\\næ–¹æ¡ˆ2: ä¿®æ”¹è®­ç»ƒå‚æ•°\")\n",
    "        print(\"   è¿è¡Œè®­ç»ƒæ—¶ä½¿ç”¨:\")\n",
    "        print(\"   python finetune_mmaudio_m1.py \\\\\")\n",
    "        print(\"     --exp_id experiment_01 \\\\\")\n",
    "        print(\"     --model small_16k \\\\\")\n",
    "        print(\"     --batch_size 1 \\\\\")\n",
    "        print(\"     --gradient_accumulation_steps 4\")\n",
    "        print(\"\\næ–¹æ¡ˆ3: ä½¿ç”¨æ•°æ®å¢å¼º\")\n",
    "        print(\"   å¯ä»¥é€šè¿‡å¤åˆ¶ç°æœ‰æ ·æœ¬æ¥åˆ›å»ºæ›´å¤šè®­ç»ƒæ•°æ®\")\n",
    "        print(\"   (æ³¨æ„: è¿™åªæ˜¯ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œä¸æ¨èç”¨äºå®é™…è®­ç»ƒ)\")\n",
    "\n",
    "def create_dummy_samples(num_copies=5):\n",
    "    \"\"\"åˆ›å»ºè™šæ‹Ÿæ ·æœ¬ç”¨äºæµ‹è¯•ï¼ˆä¸æ¨èç”¨äºå®é™…è®­ç»ƒï¼‰\"\"\"\n",
    "    print_header(\"ğŸ”§ åˆ›å»ºæµ‹è¯•æ ·æœ¬\")\n",
    "    \n",
    "    memmap_dir = Path('./output/memmap/vgg-my_videos')\n",
    "    if not memmap_dir.exists():\n",
    "        print(\"âŒ æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ•°æ®\")\n",
    "        return\n",
    "    \n",
    "    print(\"âš ï¸  æ³¨æ„: è¿™åªæ˜¯ç”¨äºæµ‹è¯•ï¼Œä¸åº”ç”¨äºå®é™…è®­ç»ƒ!\")\n",
    "    response = input(\"æ˜¯å¦ç»§ç»­? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        return\n",
    "    \n",
    "    # åŠ è½½åŸå§‹æ•°æ®\n",
    "    tensor_dict = td.TensorDict.load_memmap(memmap_dir)\n",
    "    \n",
    "    # å¤åˆ¶æ•°æ®\n",
    "    new_data = {}\n",
    "    for key in tensor_dict.keys():\n",
    "        original = tensor_dict[key]\n",
    "        # é‡å¤æ•°æ®\n",
    "        new_data[key] = original.repeat(num_copies, *[1] * (len(original.shape) - 1))\n",
    "    \n",
    "    # ä¿å­˜æ–°æ•°æ®\n",
    "    new_tensor_dict = td.TensorDict(new_data)\n",
    "    new_memmap_dir = Path('./output/memmap/vgg-my_videos_augmented')\n",
    "    new_tensor_dict.memmap_(new_memmap_dir)\n",
    "    \n",
    "    # æ›´æ–°TSVæ–‡ä»¶\n",
    "    tsv_path = Path('./output/memmap/vgg-my_videos.tsv')\n",
    "    if tsv_path.exists():\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        # å¤åˆ¶è¡Œ\n",
    "        new_df = pd.concat([df] * num_copies, ignore_index=True)\n",
    "        # æ›´æ–°IDä»¥é¿å…é‡å¤\n",
    "        for i in range(len(new_df)):\n",
    "            if i >= len(df):\n",
    "                new_df.loc[i, 'id'] = f\"{new_df.loc[i, 'id']}_copy_{i//len(df)}\"\n",
    "        \n",
    "        new_tsv_path = Path('./output/memmap/vgg-my_videos_augmented.tsv')\n",
    "        new_df.to_csv(new_tsv_path, sep='\\t', index=False)\n",
    "        \n",
    "        print(f\"âœ… åˆ›å»ºäº† {len(new_df)} ä¸ªæ ·æœ¬\")\n",
    "        print(f\"   æ•°æ®ä¿å­˜åœ¨: {new_memmap_dir}\")\n",
    "        print(f\"   TSVæ–‡ä»¶: {new_tsv_path}\")\n",
    "        print(\"\\nè®­ç»ƒæ—¶ä½¿ç”¨:\")\n",
    "        print(\"   python finetune_mmaudio_m1.py \\\\\")\n",
    "        print(\"     --data_dir ./output/memmap \\\\\")\n",
    "        print(\"     --tsv_file ./output/memmap/vgg-my_videos_augmented.tsv \\\\\")\n",
    "        print(\"     --exp_id test_augmented\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # è¿è¡Œæ£€æŸ¥\n",
    "    check_training_data()\n",
    "    \n",
    "    # å¦‚æœåªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œè¯¢é—®æ˜¯å¦åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"å¦‚æœéœ€è¦åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œè¯·è¿è¡Œ:\")\n",
    "    print(\"python check_dataset.py --create-dummy\")\n",
    "    \n",
    "    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == '--create-dummy':\n",
    "        create_dummy_samples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
